{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Airflow Interview Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "**What is Apache Airflow? Why is it used?**\n",
    "\n",
    "Apache Airflow is an **open-source workflow orchestration tool** that allows developers to programmatically **author, schedule, and monitor workflows or data pipelines**. Workflows in Airflow are defined as Directed Acyclic Graphs (DAGs), which represent the series of tasks to be executed and their dependencies.\n",
    "\n",
    "Airflow is widely used in data engineering and ETL processes to:\n",
    "- Schedule workflows: Automate the execution of workflows at specific times or intervals.\n",
    "- Monitor workflows: Provide detailed logging and visual interfaces to track task progress.\n",
    "- Handle dependencies: Manage complex task dependencies and ensure tasks are executed in the correct order.\n",
    "- Retry and recovery: Automatically retry failed tasks and recover from errors.\n",
    "\n",
    "Its flexibility, extensibility (through Python-based DAG definitions), and support for integration with external systems (e.g., databases, APIs, cloud services) make it a powerful tool for orchestrating data workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "**What are DAGs in Airflow? How do they work?**\n",
    "\n",
    "A Directed Acyclic Graph (DAG) is the core concept in Apache Airflow. It is a representation of a workflow or pipeline where:\n",
    "\n",
    "Nodes represent tasks.\n",
    "Edges represent dependencies between task.\n",
    "\n",
    "**DAGs in Airflow have the following characteristics:**\n",
    "\n",
    "Directed: Tasks have a defined execution order.\n",
    "Acyclic: No circular dependencies are allowed; tasks cannot point back to an earlier task.\n",
    "\n",
    "\n",
    "**How they work:**\n",
    "\n",
    "Definition: A DAG is defined in Python code using the airflow.models.DAG class. It includes task definitions and dependency relationships.\n",
    "\n",
    "Execution: The Airflow Scheduler reads DAG definitions, evaluates their schedule and queues the tasks for execution based on dependencies.\n",
    "\n",
    "Monitoring: The Web UI provides real-time updates on task statuses like running, success, failed, or skipped.\n",
    "\n",
    "**For example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "from datetime import datetime\n",
    "\n",
    "# Define a DAG\n",
    "with DAG(\n",
    "    'example_dag',\n",
    "    schedule_interval='@daily',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False\n",
    ") as dag:\n",
    "\n",
    "    # Define tasks\n",
    "    start = DummyOperator(task_id='start')\n",
    "    process = DummyOperator(task_id='process')\n",
    "    end = DummyOperator(task_id='end')\n",
    "\n",
    "    # Define dependencies\n",
    "    start >> process >> end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 3**\n",
    "\n",
    "**What is the difference between a Task and a DAG in Airflow?**\n",
    "\n",
    "**Definition:** A Task represents a single unit of work in a workflow (e.g., running a script or querying DB) while\tA DAG represents the entire workflow, composed of multiple tasks and their dependencies.\n",
    "\n",
    "**Scope:**\tA Task is an instance of an Operator (e.g., PythonOperator, BashOperator) while A DAG is a container for tasks and defines the workflow's structure and execution order.\n",
    "\n",
    "**Execution:**\tA Task executes a specific function or script.\tA DAG does not execute anything directly; it acts as a blueprint for the workflow.\n",
    "\n",
    "**Dependency:**\tTasks are linked together to define the order of execution while The DAG defines these task dependencies.\n",
    "\n",
    "In short, a Task is an action while a DAG organizes these actions into a complete workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 4** \n",
    "\n",
    "**What are the key components of Airflow architecture?**\n",
    "\n",
    "DAGs (Directed Acyclic Graphs): Represent workflows in Airflow.\n",
    "\n",
    "**Scheduler:** Manages the scheduling of tasks in DAGs, ensuring tasks are executed according to their dependencies and schedule intervals.\n",
    "\n",
    "**Executor:** Executes tasks on worker nodes. Different types include:\n",
    "- SequentialExecutor: Executes tasks one at a time.\n",
    "- LocalExecutor: Executes tasks in parallel on the local machine.\n",
    "- CeleryExecutor: Distributes tasks across multiple worker nodes.\n",
    "- KubernetesExecutor: Executes tasks in Kubernetes pods.\n",
    "\n",
    "**Metadata Database:** Stores the state of DAGs, tasks, and other Airflow configurations. Common databases used include PostgreSQL and MySQL.\n",
    "\n",
    "**Web Server:** Provides a user interface for monitoring DAGs, tasks, and their statuses.\n",
    "\n",
    "**Workers:** Nodes responsible for running the tasks assigned by the Scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 5**\n",
    "\n",
    "**What is an Operator in Airflow? Can you name a few types of Operators?**\n",
    "\n",
    "An Operator in Airflow is a building block of tasks. It defines what a task should do (e.g., execute a Python function, run a Bash script, or transfer data). Operators are designed to be reusable and modular.\n",
    "\n",
    "**Types of Operators:**\n",
    "\n",
    "1. **Action Operators:** Perform specific actions.\n",
    "- PythonOperator (executes Python code)\n",
    "- BashOperator (executes Bash scripts)\n",
    "\n",
    "2. **Transfer Operators:** Transfer data between systems.\n",
    "- S3ToGCSOperator (moves data from S3 to Google Cloud Storage)\n",
    "- MySqlToGCSOperator (exports data from MySQL to Google Cloud Storage)\n",
    "\n",
    "3. **Sensor Operators:** Wait for a condition to be met before proceeding.\n",
    "- FileSensor (waits for a file to appear)\n",
    "- ExternalTaskSensor (waits for a task in another DAG to complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 6**\n",
    "\n",
    "**What is a Task Instance? How is it different from a Task?**\n",
    "\n",
    "A Task Instance is a specific execution of a Task within a DAG run. While a Task is a blueprint (what needs to be done), a Task Instance is its runtime execution with unique metadata like execution date and state.\n",
    "\n",
    "Task: Defined in code, represents what action to perform.\n",
    "\n",
    "Task Instance: Represents a single execution of that task on a specific DAG run.\n",
    "\n",
    "**Example:** If a DAG runs daily, the same Task will have multiple Task Instances, each corresponding to a different day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 7**\n",
    "\n",
    "**What are Sensors in Airflow? Provide an example use case.**\n",
    "\n",
    "Sensors are a special type of operator in Airflow designed to wait for a condition to be met. Sensors can pause a workflow until the specified condition, such as the presence of a file, is satisfied.\n",
    "\n",
    "**Example use case:**\n",
    "\n",
    "Use a S3KeySensor to wait for a specific file to arrive in an S3 bucket before proceeding with data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.sensors.s3_key_sensor import S3KeySensor\n",
    "from airflow import DAG\n",
    "from datetime import datetime\n",
    "\n",
    "with DAG('s3_sensor_dag', start_date=datetime(2024, 1, 1)) as dag:\n",
    "    wait_for_file = S3KeySensor(\n",
    "        task_id='wait_for_file',\n",
    "        bucket_name='my-bucket',\n",
    "        bucket_key='data/file.csv',\n",
    "        aws_conn_id='my_aws_connection',\n",
    "        timeout=3600\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 8**\n",
    "\n",
    "**How do you backfill a DAG in Airflow?**\n",
    "\n",
    "Backfilling in Airflow involves running a DAG for past dates that it missed due to being disabled or not existing at the time.\n",
    "\n",
    "To backfill:\n",
    "\n",
    "Enable catchup=True in the DAG definition. This tells Airflow to execute the DAG for all intervals since the start_date.\n",
    "\n",
    "Use the Airflow CLI:\n",
    "\n",
    "`airflow dags backfill -s 2023-01-01 -e 2023-12-31 my_dag_id`\n",
    "\n",
    "This runs the DAG for all scheduled intervals between the start and end dates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 9**\n",
    "\n",
    "**What is the purpose of the airflow.cfg file?**\n",
    "\n",
    "The airflow.cfg file is the main configuration file for Airflow. It contains settings for:\n",
    "- Core settings (e.g., default timezone, DAG folder path)\n",
    "- Executors (e.g., type of executor, parallelism)\n",
    "- Logging (e.g., log file location)\n",
    "- Database connections (e.g., metadata DB URI)\n",
    "- Security (e.g., authentication method)\n",
    "\n",
    "Example snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "[core]\n",
    "dags_folder = /path/to/dags\n",
    "executor = CeleryExecutor\n",
    "sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@localhost:5432/airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 10**\n",
    "\n",
    "**What are the different types of task dependencies in Airflow?**\n",
    "\n",
    "Dependencies define the execution order of tasks. Common types include:\n",
    "\n",
    "Sequential: A task depends on another.\n",
    "\n",
    "`task1 >> task2`\n",
    "\n",
    "Parallel: Multiple tasks run simultaneously.\n",
    "\n",
    "`task1 >> [task2, task3]`\n",
    "\n",
    "Cross dependencies:\n",
    "\n",
    "`task1 >> task2 >> task4`\n",
    "\n",
    "`task3 >> task4`\n",
    "\n",
    "Dynamic dependencies: Set programmatically within a loop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INTERMEDIATE QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 1**\n",
    "\n",
    "How would you schedule a DAG to run at specific intervals? Explain cron syntax in the context of Airflow.\n",
    "\n",
    "In Airflow, DAG schedules are defined using the schedule_interval parameter. You can specify:\n",
    "\n",
    "Predefined intervals:\n",
    "\n",
    "`@daily`: Runs once a day.\n",
    "\n",
    "`@hourly`: Runs every hour.\n",
    "\n",
    "`@once`: Runs a single time when the DAG is triggered.\n",
    "\n",
    "`@weekly`, @monthly: Other predefined options.\n",
    "\n",
    "`Cron expressions`: Provide precise scheduling flexibility using cron syntax:\n",
    "\n",
    "Format: minute hour day_of_month month day_of_week\n",
    "\n",
    "Example: 0 12 * * * runs the DAG every day at 12:00 PM.\n",
    "\n",
    "Example DAG using cron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "from datetime import datetime\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"cron_example\",\n",
    "    schedule_interval=\"30 9 * * 1-5\",  # 9:30 AM, Monday to Friday\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False\n",
    ") as dag:\n",
    "    start = DummyOperator(task_id=\"start\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 2**\n",
    "\n",
    "What is the role of the Airflow Scheduler? How does it differ from the Worker?\n",
    "\n",
    "Scheduler:\n",
    "\n",
    "The Scheduler is responsible for monitoring DAGs, triggering task executions at the correct time and managing task dependencies.\n",
    "\n",
    "- It decides which tasks are ready to run and sends them to the executor for execution.\n",
    "\n",
    "- It does not execute tasks directly.\n",
    "\n",
    "Worker:\n",
    "\n",
    "- The Worker executes the tasks assigned by the Scheduler.\n",
    "\n",
    "- Depending on the executor type, workers may run locally (LocalExecutor) or on distributed nodes (CeleryExecutor, KubernetesExecutor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 3**\n",
    "What are XComs in Airflow? How are they used?\n",
    "\n",
    "XComs (Cross-communications) allow tasks in Airflow to share data between one another. They are:\n",
    "\n",
    "- Key-value-based: Data is stored using a key and retrieved using the same key.\n",
    "- Stored in the metadata database.\n",
    "\n",
    "Use cases:\n",
    "\n",
    "- Passing data from one task to another.\n",
    "- Storing intermediate results.\n",
    "\n",
    "Example: Task 1 pushes data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "def push_data(**kwargs):\n",
    "    kwargs['ti'].xcom_push(key='example_key', value='hello_world')\n",
    "\n",
    "task1 = PythonOperator(task_id='push_task', python_callable=push_data, provide_context=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Task 2 pulls data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_data(**kwargs):\n",
    "    value = kwargs['ti'].xcom_pull(task_ids='push_task', key='example_key')\n",
    "    print(f\"Value: {value}\")\n",
    "\n",
    "task2 = PythonOperator(task_id='pull_task', python_callable=pull_data, provide_context=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 4**\n",
    "\n",
    "How do you handle task retries and failure in Airflow?\n",
    "\n",
    "Airflow provides configurable options for handling task retries and failures:\n",
    "\n",
    "Retries: Specify the number of retry attempts for a task using the retries parameter.\n",
    "\n",
    "Retry delay: Set a delay between retries using the retry_delay parameter.\n",
    "\n",
    "Custom failure behavior: Use on_failure_callback to define custom logic (e.g., send an alert).\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def my_task():\n",
    "    raise Exception(\"Simulating failure\")\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"retry_example\",\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    schedule_interval=\"@daily\",\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "    retry_task = PythonOperator(\n",
    "        task_id=\"retry_task\",\n",
    "        python_callable=my_task,\n",
    "        retries=3,  # Retry 3 times\n",
    "        retry_delay=timedelta(minutes=5),  # Wait 5 minutes between retries\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 5**\n",
    "\n",
    "What is the difference between @daily, @hourly, and @once schedule intervals?\n",
    "\n",
    "`@daily`: Runs the DAG once a day, at midnight by default.\n",
    "\n",
    "`@hourly`: Runs the DAG once every hour.\n",
    "\n",
    "`@once`: Executes the DAG only once, immediately when triggered.\n",
    "\n",
    "Example Comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAG(dag_id=\"daily_example\", schedule_interval=\"@daily\", ...)\n",
    "DAG(dag_id=\"hourly_example\", schedule_interval=\"@hourly\", ...)\n",
    "DAG(dag_id=\"once_example\", schedule_interval=\"@once\", ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 6**\n",
    "\n",
    "**What is the purpose of the Airflow CLI? Can you give examples of common commands?**\n",
    "\n",
    "The Airflow CLI allows users to manage DAGs, tasks, and configurations directly from the command line.\n",
    "\n",
    "Common commands:\n",
    "\n",
    "List all DAGs:\n",
    "\n",
    "`airflow dags list`\n",
    "\n",
    "Trigger a DAG manually:\n",
    "\n",
    "`airflow dags trigger my_dag_id`\n",
    "\n",
    "Pause a DAG:\n",
    "\n",
    "`airflow dags pause my_dag_id`\n",
    "\n",
    "View task logs:\n",
    "\n",
    "`airflow tasks logs my_dag_id my_task_id 2024-01-01T00:00:00`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 7**\n",
    "\n",
    "**How do you set up Airflow connections for external systems like databases or APIs?**\n",
    "\n",
    "Airflow uses Connections to securely store credentials and configuration for external systems. These can be managed via the Airflow UI or CLI.\n",
    "\n",
    "Steps to create a connection:\n",
    "\n",
    "Navigate to Admin > Connections in the Airflow UI.\n",
    "\n",
    "Click Create.\n",
    "\n",
    "Fill in the details:\n",
    "\n",
    "Connection ID: Unique name for the connection.\n",
    "\n",
    "Connection Type: (e.g., Postgres, S3, HTTP).\n",
    "\n",
    "Host, Port, Login, Password, Schema: As applicable.\n",
    "\n",
    "**Example: Setting up an HTTP connection:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "airflow connections add \\\n",
    "    --conn-id my_http_conn \\\n",
    "    --conn-type HTTP \\\n",
    "    --conn-host https://api.example.com \\\n",
    "    --conn-login my_username \\\n",
    "    --conn-password my_password\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 8**\n",
    "\n",
    "**What is a SubDagOperator? When would you use it?**\n",
    "\n",
    "The SubDagOperator is used to embed a smaller DAG within a parent DAG. It is useful for modularizing workflows with repetitive patterns.\n",
    "\n",
    "**Limitations:**\n",
    "\n",
    "SubDAGs are executed as separate DAGs, which can add overhead.\n",
    "\n",
    "They are now largely replaced by Task Groups in Airflow 2.x for better performance and simplicity.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "from airflow.operators.subdag import SubDagOperator\n",
    "\n",
    "def subdag(parent_dag_id, child_dag_id, args):\n",
    "    with DAG(dag_id=f\"{parent_dag_id}.{child_dag_id}\", default_args=args) as sub_dag:\n",
    "        task1 = DummyOperator(task_id=\"task1\")\n",
    "        task2 = DummyOperator(task_id=\"task2\")\n",
    "        task1 >> task2\n",
    "    return sub_dag\n",
    "\n",
    "with DAG(dag_id=\"parent_dag\", start_date=datetime(2024, 1, 1)) as dag:\n",
    "    subdag_task = SubDagOperator(\n",
    "        task_id=\"subdag_task\",\n",
    "        subdag=subdag(\"parent_dag\", \"child_dag\", args={}),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 9**\n",
    "\n",
    "Explain how Airflow handles logging and where logs are stored.\n",
    "\n",
    "Airflow generates logs for each task instance, which are accessible via the Web UI or the file system.\n",
    "\n",
    "1. Logging configuration: Defined in the airflow.cfg file under the [logging] section.\n",
    "\n",
    "- Local logs: Stored in the logs/ directory by default.\n",
    "- Remote logs: Can be stored in cloud storage (e.g., S3, GCS).\n",
    "\n",
    "2. Log per task instance: Each task execution generates its own log file.\n",
    "\n",
    "Example logging configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "[logging]\n",
    "base_log_folder = /path/to/airflow/logs\n",
    "remote_logging = True\n",
    "remote_log_conn_id = my_s3_conn\n",
    "remote_base_log_folder = s3://my-airflow-logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 10**\n",
    "\n",
    "**What are the pros and cons of running Airflow in standalone mode vs. distributed mode?**\n",
    "\n",
    "- **Use Case** - Standalone is ideal for development and small workflows while distributed is suitable for production and large workflows.\n",
    "- **Setup Complexity** - Standalone is easy to set up (single machine) while Distributed complex setup (requires multiple nodes and external database).\n",
    "- **Scalability**- Standalone is limited to a single machine's resources while Distributed scales horizontally with Celery or Kubernetes Executors.\n",
    "- **Performance**- Standalone can handle only a limited number of tasks while Distributed can handles high workloads with distributed execution across workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADVANCED QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 1**\n",
    "\n",
    "**How do you scale Airflow for high availability and large workflows?**\n",
    "\n",
    "To scale Airflow for high availability (HA) and large workflows you need to focus on redundancy, resource allocation, and distributed execution:\n",
    "\n",
    "1. Use a Distributed Executor:\n",
    "\n",
    "- CeleryExecutor: Distributes tasks across multiple worker nodes using a message broker like RabbitMQ or Redis.\n",
    "- KubernetesExecutor: Dynamically scales by running tasks in isolated Kubernetes pods.\n",
    "- Advantages: Tasks are distributed, making it easier to handle heavy workflows.\n",
    "\n",
    "2. Multiple Schedulers:\n",
    "\n",
    "- Running multiple schedulers ensures that if one fails, others can take over, reducing downtime.\n",
    "- Airflow 2.x supports multiple active schedulers.\n",
    "\n",
    "3. Metadata Database:\n",
    "\n",
    "- Use a high-availability database like PostgreSQL or MySQL with replication.\n",
    "- Optimize database performance by enabling connection pooling (e.g., using pgbouncer for PostgreSQL).\n",
    "\n",
    "4. Load Balancing:\n",
    "\n",
    "- Use a load balancer in front of the web server to distribute UI traffic.\n",
    "\n",
    "5. Storage:\n",
    "\n",
    "- Centralize log storage in a shared location, such as S3, GCS, or an NFS mount, to make logs accessible to all nodes.\n",
    "\n",
    "6. Monitoring:\n",
    "\n",
    "- Use tools like Prometheus and Grafana to monitor system performance and alert on failures.\n",
    "\n",
    "**Example Architecture:**\n",
    "\n",
    "- Airflow Web Server: Multiple instances behind a load balancer.\n",
    "- Scheduler: Two or more instances for redundancy.\n",
    "- Workers: Multiple worker nodes (e.g., in Celery or Kubernetes mode).\n",
    "- Message Broker: Redis or RabbitMQ for task queuing.\n",
    "- Metadata DB: Highly available PostgreSQL cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 2**\n",
    "\n",
    "**What are Task Groups in Airflow 2.x? How do they differ from SubDAGs?**\n",
    "\n",
    "Task Groups were introduced in Airflow 2.x as a way to visually group tasks in the UI without the overhead of creating SubDAGs.\n",
    "\n",
    "- Task Groups:\n",
    "    - Lightweight and simple.\n",
    "    - Used for grouping tasks logically, improving readability in the UI.\n",
    "    - Tasks remain part of the parent DAG.\n",
    "\n",
    "- SubDAGs:\n",
    "    - Executed as separate DAGs, which can cause performance issues.\n",
    "    - Requires a SubDagOperator.\n",
    "    - Deprecated in favor of Task Groups.\n",
    "\n",
    "Example Task Group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.utils.task_group import TaskGroup\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "from airflow import DAG\n",
    "from datetime import datetime\n",
    "\n",
    "with DAG(dag_id=\"task_group_example\", start_date=datetime(2024, 1, 1)) as dag:\n",
    "    start = DummyOperator(task_id=\"start\")\n",
    "\n",
    "    with TaskGroup(group_id=\"group1\") as group1:\n",
    "        task1 = DummyOperator(task_id=\"task1\")\n",
    "        task2 = DummyOperator(task_id=\"task2\")\n",
    "        task1 >> task2\n",
    "\n",
    "    end = DummyOperator(task_id=\"end\")\n",
    "\n",
    "    start >> group1 >> end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 3**\n",
    "\n",
    "**How would you secure an Airflow deployment in a production environment?**\n",
    "\n",
    "Securing Airflow in production involves addressing access control, data security, and network security:\n",
    "\n",
    " - Authentication and Authorization:\n",
    "    - Enable RBAC (Role-Based Access Control) to manage user permissions.\n",
    "    - Use OAuth, LDAP, or other authentication backends.\n",
    "\n",
    "- Secure Connections:\n",
    "    - Use HTTPS for the web server.\n",
    "    - Store sensitive connection details (e.g., passwords, keys) in a secrets manager like AWS Secrets Manager, HashiCorp Vault, or Azure Key Vault.\n",
    "\n",
    "- Database Security:\n",
    "    - Restrict database access to only the Airflow service account.\n",
    "    - Use encryption for the metadata database.\n",
    "\n",
    "- Network Security:\n",
    "    - Deploy Airflow in a private network or behind a VPN.\n",
    "    - Limit access to the web server and API endpoints using a firewall.\n",
    "\n",
    "- Environment Isolation:\n",
    "    - Use containerized deployments (e.g., Docker, Kubernetes) to isolate Airflow processes.\n",
    "\n",
    "- Audit Logging:\n",
    "    - Enable logging for all API requests and user actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 4**\n",
    "How does Airflow integrate with cloud services like AWS, GCP, or Azure?\n",
    "\n",
    "Airflow provides pre-built hooks and operators for seamless integration with cloud services.\n",
    "\n",
    "**AWS:**\n",
    "- Hooks: `S3Hook`, `RedshiftHook`\n",
    "- Operators: `S3ToRedshiftOperator`, `S3FileTransformOperator`\n",
    "\n",
    "Example: Copying files to S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.providers.amazon.aws.operators.s3 import S3CreateObjectOperator\n",
    "task = S3CreateObjectOperator(\n",
    "    task_id=\"upload_to_s3\",\n",
    "    aws_conn_id=\"my_aws_conn\",\n",
    "    bucket_name=\"my_bucket\",\n",
    "    object_key=\"path/to/file\",\n",
    "    data=\"Sample Data\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GCP:**\n",
    "\n",
    "- Hooks: `BigQueryHook`, `GCSHook`\n",
    "- Operators: `BigQueryInsertJobOperator`, `GCSToBigQueryOperator`\n",
    "\n",
    "Example: Loading data into BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator\n",
    "task = BigQueryInsertJobOperator(\n",
    "    task_id=\"load_bq\",\n",
    "    gcp_conn_id=\"my_gcp_conn\",\n",
    "    configuration={\n",
    "        \"query\": {\"query\": \"SELECT * FROM dataset.table\", \"useLegacySql\": False}\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Azure:**\n",
    "\n",
    "- Hooks: `AzureBlobStorageHook`\n",
    "- Operators: `AzureDataFactoryRunPipelineOperator`, `AzureBlobStorageDownloadOperator`\n",
    "\n",
    "Example: Loading data into Azure Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.providers.microsoft.azure.operators.azure_blob import AzureBlobStorageCreateBlobOperator\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the DAG\n",
    "with DAG(\n",
    "    dag_id='upload_to_azure_blob',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    schedule_interval='@daily',\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "\n",
    "    # Upload a file to Azure Blob Storage\n",
    "    upload_to_blob = AzureBlobStorageCreateBlobOperator(\n",
    "        task_id='upload_to_blob',\n",
    "        container_name='my-container',  # Replace with your container name\n",
    "        blob_name='data/file.csv',      # Path to store the file in the container\n",
    "        data='Sample data for Azure Blob Storage',  # File content (use a path for larger files)\n",
    "        azure_blob_conn_id='azure_blob_conn',       # Azure connection ID\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 5**\n",
    "\n",
    "What is the role of the Celery Executor? How does it compare to the Local Executor and Kubernetes Executor?\n",
    "The Celery Executor allows Airflow to run tasks in a distributed manner across multiple worker nodes.\n",
    "\n",
    "Executor\tCeleryExecutor\tLocalExecutor\tKubernetesExecutor\n",
    "- **Execution Mode**: CeleryExecutor distributes tasks across multiple nodes, LocalExecutor runs tasks locally on the same node, KubernetesExecutors runs tasks in Kubernetes pods.\n",
    "- **Scalability**: CeleryExecutor is highly scalable with more worker nodes, LocalExecutor is limited to the local machine's capacity,\tKubernetesExecutor is dynamically scalable with Kubernetes.\n",
    "- **Setup Complexity**: CeleryExecutor requires a message broker (Redis or RabbitMQ), LocalExecutor requires minimal setup.\tKubernetesExecutor requires complex setup (requires Kubernetes).\n",
    "- **Use Case**: CeleryExecutor uses medium to large workflows.\tLocalExecutor uses small to medium workflows.\tKubernetesExecutor uses Cloud-native, highly dynamic workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 6**\n",
    "\n",
    "**How would you debug a DAG that is stuck in a running state?**\n",
    "\n",
    "__Steps to debug:__\n",
    "\n",
    "1. Check Task Logs:\n",
    "    - Inspect the logs for each task instance to identify errors or bottlenecks.\n",
    "\n",
    "2. Monitor Worker and Scheduler:\n",
    "\n",
    "    - Ensure that workers are running and connected to the Scheduler.\n",
    "    - Check for resource exhaustion (e.g., CPU, memory).\n",
    "\n",
    "3. Verify Dependencies:\n",
    "\n",
    "    - Look for cyclic dependencies or improperly set trigger_rule parameters.\n",
    "\n",
    "4. Database Check:\n",
    "\n",
    "    - Query the metadata database to check the status of the stuck task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT dag_id, task_id, state FROM task_instance WHERE state = 'running';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Restart Components:\n",
    "\n",
    "    - Restart the Scheduler or Worker processes if they appear unresponsive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 7**\n",
    "\n",
    "**What is the purpose of the trigger_rule parameter? Can you give an example?**\n",
    "\n",
    "The trigger_rule parameter defines how a task is triggered based on the state of its upstream tasks. Common values:\n",
    "\n",
    "- all_success (default): Task triggers if all upstream tasks succeed.\n",
    "- all_failed: Task triggers if all upstream tasks fail.\n",
    "- one_success: Task triggers if at least one upstream task succeeds.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators.dummy import DummyOperator\n",
    "from airflow import DAG\n",
    "from datetime import datetime\n",
    "\n",
    "with DAG(dag_id=\"trigger_rule_example\", start_date=datetime(2024, 1, 1)) as dag:\n",
    "    task1 = DummyOperator(task_id=\"task1\")\n",
    "    task2 = DummyOperator(task_id=\"task2\")\n",
    "    task3 = DummyOperator(task_id=\"task3\", trigger_rule=\"one_success\")\n",
    "\n",
    "    task1 >> task3\n",
    "    task2 >> task3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, task3 will execute as soon as either task1 or task2 succeeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 8**\n",
    "\n",
    "**Explain the difference between depends_on_past and wait_for_downstream parameters.**\n",
    "\n",
    "**depends_on_past:**\n",
    "\n",
    "- Ensures that a task only runs if the same task from the previous DAG run has completed successfully.\n",
    "- Use case: Sequential processing of daily data.\n",
    "\n",
    "**wait_for_downstream:**\n",
    "\n",
    "- Ensures that a task waits for all downstream tasks from the previous DAG run to finish before starting.\n",
    "- Use case: Avoid overlapping data dependencies across runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 9**\n",
    "\n",
    "**What is the purpose of Airflow's REST API? How would you use it?**\n",
    "\n",
    "The REST API allows programmatic interaction with Airflow to:\n",
    "- Trigger DAGs.\n",
    "- Retrieve DAG and task statuses.\n",
    "- Manage connections and variables.\n",
    "\n",
    "Example: Triggering a DAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "curl -X POST \\\n",
    "  \"http://localhost:8080/api/v1/dags/my_dag_id/dagRuns\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"conf\": {}, \"dag_run_id\": \"manual_run_1\"}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 10**\n",
    "\n",
    "**How do you implement dynamic task generation in Airflow? Provide a code example.**\n",
    "\n",
    "Dynamic task generation creates tasks programmatically based on external inputs, such as a list of files or database records.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators.python import PythonOperator\n",
    "from airflow import DAG\n",
    "from datetime import datetime\n",
    "\n",
    "def process_file(file_name):\n",
    "    print(f\"Processing file: {file_name}\")\n",
    "\n",
    "file_list = [\"file1.csv\", \"file2.csv\", \"file3.csv\"]\n",
    "\n",
    "with DAG(dag_id=\"dynamic_task_example\", start_date=datetime(2024, 1, 1)) as dag:\n",
    "    for file in file_list:\n",
    "        task = PythonOperator(\n",
    "            task_id=f\"process_{file}\",\n",
    "            python_callable=process_file,\n",
    "            op_args=[file]\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
